# -*- coding: utf-8 -*-
"""homework4.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vUR22gGHRQ_tdOl-06ZdTuvXnfsUHyrv
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from IPython.display import Markdown, display
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import numpy as np
from sklearn import metrics

seed = 7
np.random.seed(seed)

def printmd(string):
    display(Markdown(string))
    
    
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import cross_val_score

from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize


# %  matplotlib inline

#adult = pd.read_csv('adult.csv')

column_names = ['age', 'workclass', 'fnlwgt', 'education', 'educational-num','marital-status', 'occupation', 'relationship', 'race', 'gender','capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income']

train = pd.read_csv('adult_data.txt', sep=",\s", header=None, names = column_names, engine = 'python')
test = pd.read_csv('adult_test.txt', sep=",\s", header=None, names = column_names, engine = 'python')
test['income'].replace(regex=True,inplace=True,to_replace=r'\.',value=r'')


adult = pd.concat([test,train])
adult.reset_index(inplace = True, drop = True)

"""---
# 1. Preliminary Data Analysis
"""

# Setting all the categorical columns to type category
for col in set(adult.columns) - set(adult.describe().columns):
    adult[col] = adult[col].astype('category')
    
printmd('## 1.1. Columns and their types')
print(adult.info())

# Top 5 records
printmd('## 1.2. Data')
adult.head()

printmd('## 1.3. Summary Statistics')

adult.describe()

printmd('## 1.4. Missing values')
for i,j in zip(adult.columns,(adult.values.astype(str) == '?').sum(axis = 0)):
    if j > 0:
        printmd(str(i) + ': ' + str(j) + ' records')

"""### Treating Missing Values by predicting them

I fill the missing values in each of the three columns by predicting their values. For each of the three columns, I use all the attributes (including 'income') as independent variables and treat that column as the dependent variable, making it a multi-class classification task. I use three classification algorithms, namely, logistic regression, decision trees and random forest to predict the class when the value is missing (in this case a '?'). I then take a majority vote amongst the three classifiers to be the class of the missing value. In case of a tie, I pick the majority class of that column using the entire dataset.
"""

# Create one hot encoding of the categorical columns in the data frame.
def oneHotCatVars(df, df_cols):
    
    df_1 = adult_data = df.drop(columns = df_cols, axis = 1)
    df_2 = pd.get_dummies(df[df_cols])
    
    return (pd.concat([df_1, df_2], axis=1, join='inner'))

printmd('### 1.4.1. Filling in missing values for Attribute workclass')

test_data = adult[(adult.workclass.values == '?')].copy()
test_label = test_data.workclass

train_data = adult[(adult.workclass.values != '?')].copy()
train_label = train_data.workclass

test_data.drop(columns = ['workclass'], inplace = True)
train_data.drop(columns = ['workclass'], inplace = True)

train_data = oneHotCatVars(train_data, train_data.select_dtypes('category').columns)
test_data = oneHotCatVars(test_data, test_data.select_dtypes('category').columns)

log_reg = LogisticRegression()
log_reg.fit(train_data, train_label)
log_reg_pred = log_reg.predict(test_data)


clf = tree.DecisionTreeClassifier()
clf = clf.fit(train_data, train_label)
clf_pred = clf.predict(test_data)

r_forest = RandomForestClassifier(n_estimators=10)
r_forest.fit(train_data, train_label)
r_forest_pred = r_forest.predict(test_data)

majority_class = adult.workclass.value_counts().index[0]

pred_df =  pd.DataFrame({'RFor': r_forest_pred, 'DTree' : clf_pred, 'LogReg' : log_reg_pred})
overall_pred = pred_df.apply(lambda x: x.value_counts().index[0] if x.value_counts()[0] > 1 else majority_class, axis = 1)

adult.loc[(adult.workclass.values == '?'),'workclass'] = overall_pred.values
print(adult.workclass.value_counts())
print(adult.workclass.unique())

printmd('### 1.4.2. Filling in missing values for Occupation occupation')

test_data = adult[(adult.occupation.values == '?')].copy()
test_label = test_data.occupation

train_data = adult[(adult.occupation.values != '?')].copy()
train_label = train_data.occupation

test_data.drop(columns = ['occupation'], inplace = True)
train_data.drop(columns = ['occupation'], inplace = True)

train_data = oneHotCatVars(train_data, train_data.select_dtypes('category').columns)
test_data = oneHotCatVars(test_data, test_data.select_dtypes('category').columns)

log_reg = LogisticRegression()
log_reg.fit(train_data, train_label)
log_reg_pred = log_reg.predict(test_data)


clf = tree.DecisionTreeClassifier()
clf = clf.fit(train_data, train_label)
clf_pred = clf.predict(test_data)

r_forest = RandomForestClassifier(n_estimators=10)
r_forest.fit(train_data, train_label)
r_forest_pred = r_forest.predict(test_data)


majority_class = adult.occupation.value_counts().index[0]

pred_df =  pd.DataFrame({'RFor': r_forest_pred, 'DTree' : clf_pred, 'LogReg' : log_reg_pred})
overall_pred = pred_df.apply(lambda x: x.value_counts().index[0] if x.value_counts()[0] > 1 else majority_class, axis = 1)

adult.loc[(adult.occupation.values == '?'),'occupation'] = overall_pred.values
print(adult.occupation.value_counts())
print(adult.occupation.unique())

printmd('### 1.4.3. Filling in missing values for Native Country')

test_data = adult[(adult['native-country'].values == '?')].copy()
test_label = test_data['native-country']

train_data = adult[(adult['native-country'].values != '?')].copy()
train_label = train_data['native-country']

test_data.drop(columns = ['native-country'], inplace = True)
train_data.drop(columns = ['native-country'], inplace = True)

train_data = oneHotCatVars(train_data, train_data.select_dtypes('category').columns)
test_data = oneHotCatVars(test_data, test_data.select_dtypes('category').columns)

log_reg = LogisticRegression()
log_reg.fit(train_data, train_label)
log_reg_pred = log_reg.predict(test_data)


clf = tree.DecisionTreeClassifier()
clf = clf.fit(train_data, train_label)
clf_pred = clf.predict(test_data)

r_forest = RandomForestClassifier(n_estimators=10)
r_forest.fit(train_data, train_label)
r_forest_pred = r_forest.predict(test_data)


majority_class = adult['native-country'].value_counts().index[0]

pred_df =  pd.DataFrame({'RFor': r_forest_pred, 'DTree' : clf_pred, 'LogReg' : log_reg_pred})
overall_pred = pred_df.apply(lambda x: x.value_counts().index[0] if x.value_counts()[0] > 1 else majority_class, axis = 1)

adult.loc[(adult['native-country'].values == '?'),'native-country'] = overall_pred.values
print(adult['native-country'].value_counts())
print(adult['native-country'].unique())

# Resetting the categories

adult['workclass'] = adult['workclass'].cat.remove_categories('?')
adult['occupation'] = adult['occupation'].cat.remove_categories('?')
adult['native-country'] = adult['native-country'].cat.remove_categories('?')

printmd('## 1.5. Correlation Matrix')

display(adult.corr())

printmd('We see that none of the columns are highly correlated.')

"""---
# 3. Data Transformations

## 3.1. Feature Selection
"""

# Remove education and fnlwgt
#adult.drop(columns = ['education','fnlwgt','hours-per-week'], inplace = True)

printmd('* For education level, we have 2 features that convey the same meaning, \'education\' \
        and \'educational-num\'. To avoid the effect of this attribute on the models to be \
        overstated, I am not going to use the categorical education attribute.')
printmd('* I use the categorical Hours work column and drop the \'hour-per-week\' column')
printmd('* Also, I chose not to use the \'Fnlwgt\' attribute that is used by the census, \
        as the inverse of sampling fraction adjusted for non-response and over or under sampling \
        of particular groups. This attribute does not convey individual related meaning.')

"""## 3.2 Normalization"""

printmd('## Box plot')
adult.select_dtypes(exclude = 'category').plot(kind = 'box', figsize = (10,8))

printmd ('Normalization happens on the training dataset, by removing the mean and \
        scaling to unit variance. These values are stored and then later applied  \
        to the test data before the test data is passed to the model for prediction. ')

"""---
# 4. Model Development & Classification

## 4.1. Data Preparation'

One-hot encoding is the process of representing multi-class categorical features as binary features, one for each class. Although this process increases the dimensionality of the dataset, classification algorithms tend to work better on this format of data.

I use one-hot encoding to represent all the categorical features in the dataset.
"""

# Data Prep
adult_data = adult.drop(columns = ['income'])
adult_label = adult.income


adult_cat_1hot = pd.get_dummies(adult_data.select_dtypes('category'))
adult_non_cat = adult_data.select_dtypes(exclude = 'category')

adult_data_1hot = pd.concat([adult_non_cat, adult_cat_1hot], axis=1, join='inner')

def vectorize_sequences(squences, dimension=10000):
    """
    @函数功能:将序列向量化，初始化全0的序列，在单词索引对应的位置上置1
    """
    resluts = np.zeros((len(squences), dimension))
    for i, sequence in enumerate(squences):
        resluts[i, sequence] = 1
    return resluts

def encode_label(label):
  if label == '>50K':
    return 1
  return 0

adult_label.values

yyyy = np.array( [ encode_label(i) for i in adult_label.values ] )

# Train - Test split
train_data, test_data, train_label, test_label = train_test_split(adult_data_1hot, yyyy, test_size  = 0.25)

train_data

# Normalization
from sklearn.preprocessing import StandardScaler  
scaler = StandardScaler()  

# Fitting only on training data
scaler.fit(train_data)  
train_data = scaler.transform(train_data)  

# Applying same transformation to test data
test_data = scaler.transform(test_data)

train_label

train_data[0].shape

import tensorflow as tf
import numpy as np
import pandas as pd
# from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense,Dropout

""" 构建神经网络 
不使用正则化和Droupout
使用最基础的Optimizer = SGD
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_simple = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_simple.add(Dense(128, activation='tanh', input_shape=train_data[0].shape))
model_simple.add(Dense(64, activation='tanh'))
model_simple.add(Dense(1, activation='sigmoid'))

model_simple.compile(optimizer='SGD',loss = 'binary_crossentropy',metrics=['accuracy'])
history_simple = model_simple.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_simple.history['loss'], label='train')
plt.plot(history_simple.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
不使用正则化和Droupout
使用Optimizer = adagrad
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_adagrad = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_adagrad.add(Dense(128, activation='tanh', input_shape=train_data[0].shape))
model_adagrad.add(Dense(64, activation='tanh'))
model_adagrad.add(Dense(1, activation='sigmoid'))

model_adagrad.compile(optimizer='Adagrad',loss = 'binary_crossentropy',metrics=['accuracy'])
history_adagrad = model_adagrad.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_adagrad.history['loss'], label='train')
plt.plot(history_adagrad.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
不使用正则化和Droupout
使用Optimizer = adam
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_adam = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_adam.add(Dense(128, activation='tanh', input_shape=train_data[0].shape))
model_adam.add(Dense(64, activation='tanh'))
model_adam.add(Dense(1, activation='sigmoid'))

model_adam.compile(optimizer='Adam',loss = 'binary_crossentropy',metrics=['accuracy'])
history_adam = model_adam.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_adam.history['loss'], label='train')
plt.plot(history_adam.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
不使用正则化
使用Dropout
使用Optimizer = adam
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_dropout = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_dropout.add(Dense(128, activation='tanh', input_shape=train_data[0].shape))
model_dropout.add(Dense(64, activation='tanh'))
model_dropout.add(Dropout(0.25))
model_dropout.add(Dense(1, activation='sigmoid'))

model_dropout.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])
history_dropout = model_dropout.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_dropout.history['loss'], label='train')
plt.plot(history_dropout.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
使用l2正则化不使用Dropout
使用Optimizer = adam
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_l2 = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_l2.add(Dense(128, activation='tanh', input_shape=train_data[0].shape,kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model_l2.add(Dense(64, activation='tanh'))
model_l2.add(Dense(1, activation='sigmoid'))

model_l2.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])
history_l2 = model_l2.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_l2.history['loss'], label='train')
plt.plot(history_l2.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
使用l1正则化不使用Dropout
使用Optimizer = adam
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_l1 = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_l1.add(Dense(128, activation='tanh', input_shape=train_data[0].shape,kernel_regularizer=tf.keras.regularizers.l1(0.01)))
model_l1.add(Dense(64, activation='tanh'))
model_l1.add(Dense(1, activation='sigmoid'))

model_l1.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])
history_l1 = model_l1.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_l1.history['loss'], label='train')
plt.plot(history_l1.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
使用l1正则化不使用Dropout
使用Optimizer = adagrad
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model_l1_Adagrad = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model_l1_Adagrad.add(Dense(128, activation='tanh', input_shape=train_data[0].shape,kernel_regularizer=tf.keras.regularizers.l1(0.01)))
model_l1_Adagrad.add(Dense(64, activation='tanh'))
model_l1_Adagrad.add(Dense(1, activation='sigmoid'))

model_l1_Adagrad.compile(optimizer='Adagrad',loss = 'binary_crossentropy',metrics=['accuracy'])
history_l1_Adagrad = model_l1_Adagrad.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history_l1_Adagrad.history['loss'], label='train')
plt.plot(history_l1_Adagrad.history['val_loss'], label='test')
plt.legend()
plt.show()

""" 构建神经网络 
使用l2_l1正则化
使用Dropout
使用Optimizer = adam
loss = 'binary_crossentropy'
最后输出层使用sigmoid激活函数
"""
model = Sequential()
# keras.regularizers.l1(lambda)
# keras.regularizers.l2(lambda)
# keras.regularizers.l1_l2(l1=lambda1, l2=lambda2)
model.add(Dense(128, activation='tanh', input_shape=train_data[0].shape,kernel_regularizer=tf.keras.regularizers.l1_l2(0.05,0.05)))
model.add(Dense(64, activation='tanh'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])
history = model.fit(train_data, train_label, epochs=50, batch_size=16, validation_data=(test_data, test_label))

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

salary_pre = model.predict(test_data)
for i,j in zip(salary_pre , test_label):
  if j == 1:
    print(i,j,end=' ')
    # input(   )



"""---
# 5. Conclusion

I choose **Adaboost** model as my preferred my approach. The Adaboost model not only has the **highest accuracy**, but also has the **highest precision and F-measure** of all the models developed as a part of this analysis. The advantages of using Adaboost over other models is that they are very simple to implement. Since they are made up of weak individual learners, they are less susceptible to overfitting. However, Adaboost is sensitive to noisy data and outliers.
"""
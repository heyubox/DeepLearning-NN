# -*- coding: utf-8 -*-
"""homework1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m6u1MPc7h82NJWqAe3z7fqNKwEaBs1d0
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf  
hello = tf.constant('Hello, Tensorflow')  
sess = tf.Session()
print(sess.run(hello))

"""---


1、导入 numpy 库  \
2、建立一个一维数组 a，初始化为[4,5,6]  \
(1) 输出 a 的类型（type）  \
(2) 输出 a 的各维度大小（shape） \
(3) 输出 a 的第一个元素\
"""

import numpy as np
a=np.array([4,5,6])
print(type(a))
print(a.shape)
print(a[0])

"""---


3、建立一个二维数组 b，初始化为[[4,5,6],[1,2,3]] \
(1) 输出 b 的各维度大小（shape） \
(2) 输出b[0,0],b[0,1],b[1,1]这三个元素
"""

b=np.array([[4,5,6],[1,2,3]])
print(b.shape)
print(b[0,0],b[0,1],b[1,1])

"""---
4、建立矩阵 \
(1) 建立一个大小为3 × 3的全 0 矩阵 c \
(2) 建立一个大小为4 × 5的全 1 矩阵 d \
(3) 建立一个大小为4 × 4的单位矩阵 e
"""

c=np.zeros((3,3))
d=np.ones((4,5))
e=np.eye(4)
print(c);print(d);print(e)

"""---


5、建立一个数组 f，初始化为[0,1,2,3,4,5,6,7,8,9,10,11]（arange） \
(1) 输出 f 以及 f 的各维度大小 \
"""

f=np.arange(12)
f

"""(2) 将 f 的 shape 改为3 × 4（reshape）"""

f=f.reshape(3,4)
f

"""(3) 输出 f 以及 f 的各维度大小"""

f.shape

"""(4) 输出 f 第二行（f[1,∶]）"""

f[1,:]

"""(5) 输出 f 最后两列（f[:,2:]）"""

f[:,2:]

"""(6) 输出 f 第三行最后一个元素（使用-1 表示最后一个元素）"""

f[2,-1]

"""---
二、Tensorflow 练习（提交每个练习的实现步骤描述以及下面要求提交的结果）
 
1、线性回归 \
(1) 生成训练数据
"""

num_observations=100
x=np.linspace(-3,3,num_observations)
y=np.sin(x)+np.random.uniform(-0.5,0.5,num_observations)
import matplotlib.pyplot as plt
plt.scatter(x,y)
plt.show()

"""(2)使用 tensorflow 实现线性回归模型，训练参数𝑤和𝑏。"""

n=len(x)

X = tf.placeholder("float") 
Y = tf.placeholder("float") 

W = tf.Variable(np.random.randn(), name = "W") 
b = tf.Variable(np.random.randn(), name = "b") 

learning_rate = 0.01
training_epochs = 1000

# 初始y_pred X*W
y_pred = tf.add(X*W, b) 

  
# loss函数
cost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n) 
  
# 梯度下降
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) 
  
# 初始化
init = tf.global_variables_initializer()
print('(3) 输出参数𝑤、𝑏和损失。（提交运行结果） ')
# 训练
with tf.Session() as sess: 
      
    # 初始化
    sess.run(init) 
      
    # epoch训练
    for epoch in range(training_epochs): 
          
        # 随机梯度下降，一个一个feed
        for (_x, _y) in zip(x, y): 
            sess.run(optimizer, feed_dict = {X : _x, Y : _y}) 
          
        # 显示w b
        if (epoch + 1) % 50 == 0: 
            c = sess.run(cost, feed_dict = {X : x, Y : y}) 
            print("Epoch", (epoch + 1), ": cost =", c, "W =", sess.run(W), "b =", sess.run(b)) 

    training_cost = sess.run(cost, feed_dict ={X: x, Y: y}) 
    weight = sess.run(W) 
    bias = sess.run(b)

"""(3) 输出参数𝑤、𝑏和损失。（提交运行结果） \
(4) 画出预测回归曲线以及训练数据散点图，对比回归曲线和散点图并分析原因。
（提交图片及分析）
"""

# 预测值
predictions = weight * x + bias 
print("Training cost =", training_cost, "Weight =", weight, "bias =", bias, '\n')

# 画图
plt.plot(x, y, 'ro', label ='Original data') 
plt.plot(x, predictions, label ='Fitted line') 
plt.title('Linear Regression Result') 
plt.legend() 
plt.show()

"""**答：**
从回归曲线和散点图来看，拟合效果不好，其原因在于散点数据本身不是由一个线性函数得到的，散点数据符合非线性关系

---


2、线性回归（使用多项式函数对原始数据进行变换） \
(1) 生成训练数据，数据同上 \
(2) 使用 tensorflow 实现线性回归模型，这里我们假设𝑦是𝑥的 3 次多项式
"""

num_observations=100
x=np.linspace(-3,3,num_observations)
y=np.sin(x)+np.random.uniform(-0.5,0.5,num_observations)

n=len(x)

X = tf.placeholder("float") 
Y = tf.placeholder("float") 

W1 = tf.Variable(np.random.randn(), name = "W1") 
W2 = tf.Variable(np.random.randn(), name = "W2")
W3 = tf.Variable(np.random.randn(), name = "W3")
b = tf.Variable(np.random.randn(), name = "b") 

learning_rate = 0.01
training_epochs = 1000

# 初始y_pred 
y_pred = tf.add(W1*X+W2*X**2+W3*X*X**2, b) 

  
# loss函数
cost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n) 
  
# 梯度下降
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) 
  
# 初始化
init = tf.global_variables_initializer()
print('(3) 输出参数𝑤、𝑏和损失。（提交运行结果） ')
# 训练
with tf.Session() as sess: 
      
    # 初始化
    sess.run(init) 
      
    # epoch训练
    for epoch in range(training_epochs): 
          
        # 随机梯度下降，一个一个feed
        for (_x, _y) in zip(x, y): 
            sess.run(optimizer, feed_dict = {X : _x, Y : _y}) 
          
        # 显示w b
        if (epoch + 1) % 50 == 0: 
            c = sess.run(cost, feed_dict = {X : x, Y : y}) 
            print("Epoch", (epoch + 1), ": cost =", c, "W =", sess.run([W1,W2,W3]), "b =", sess.run(b)) 

    training_cost = sess.run(cost, feed_dict ={X: x, Y: y}) 
    weight = sess.run([W1,W2,W3])
    bias = sess.run(b)

# 预测值
predictions = weight[0]*x+weight[1]*x**2+weight[2]*x*x**2 + bias 
print("Training cost =", training_cost, "Weight =", weight, "bias =", bias, '\n')

# 画图
plt.plot(x, y, 'ro', label ='Original data') 
plt.plot(x, predictions, label ='Fitted line') 
plt.title('Linear Regression Result') 
plt.legend() 
plt.show()

"""**答：**
从回归曲线和散点图来看，拟合效果挺好，其原因在于散点数据符合非线性关系，用二次函数可以拟合出较好的效果

---
3、Softmax 分类 \
(1) 获取 MNIST 数据集，每张图片像素为28 × 28
"""

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

learning_rate = 0.5
training_epochs = 2000

# X是一个Placeholder ,这个值后续再放入让TF计算，这里是一个784维，但是训练数量不确定的（用None表示）的浮点值
X = tf.placeholder("float", [None,784 ])
Y = tf.placeholder("float", [None, 10])
# 设置对应的权值和偏置的表示，Variable代表一个变量，会随着程序的生命周期做一个改变
# 需要给一个初始的值，这里都全部表示为0
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

y_pred = tf.nn.softmax(tf.matmul(X, W) + b)

#交叉熵去衡量 reduce_sum 累加
cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(y_pred), reduction_indices=[1]))
#训练的步骤，告诉tf，用梯度下降法去优化，学习率是0.5，目的是最小化交叉熵
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)
# 到目前为止，我们已经定义完了所有的步骤，下面就需要初始化这个训练步骤了，首先初始化所有变量（之前定义的变量）
init = tf.initialize_all_variables()


sess=tf.Session()
      
# 初始化
sess.run(init) 
      
#记录
cost_list=[]
accur_list=[]
best_cost=[]
best_accur=[]
correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(y_pred,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
temp_bc=1;temp_ba=0
# epoch训练
for epoch in range(training_epochs): 
          

    # 随机梯度下降，一个一个feed
    batch_xs, batch_ys = mnist.train.next_batch(100,shuffle=True)
    sess.run(train_step, feed_dict = {X : batch_xs, Y : batch_ys}) 
    c = sess.run(cross_entropy, feed_dict = {X : batch_xs, Y : batch_ys})
    a = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})   
    cost_list.append(c)
    accur_list.append(a)
    if (temp_bc>c):
      best_cost.append(c)
      temp_bc=c
    else:
      best_cost.append(temp_bc)
    if (temp_ba<a):
      best_accur.append(a)
      temp_ba=a
    else:
      best_accur.append(temp_ba)
    # 显示w b
    if (epoch + 1) % 50 == 0: 
        c = sess.run(cross_entropy, feed_dict = {X : batch_xs, Y : batch_ys}) 
        print("Epoch", (epoch + 1), ": cost =", c, "b =", sess.run(b)) 
    

print(sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))

"""---

(3) 画出训练和测试过程的准确率随迭代次数变化图，画出训练和测试过程的损
失随迭代次数变化图。（提交最终分类精度、分类损失以及两张变化图）
"""

fig,ax=plt.subplots(2,1)
ax[0].plot(range(training_epochs) ,accur_list)
ax[1].plot(range(training_epochs),cost_list)
plt.show()

fig,ax=plt.subplots(2,1)
ax[0].plot(best_accur,'r-')
ax[1].plot(best_cost,'b-')